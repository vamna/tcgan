{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import *\n",
    "import  pandas as pd\n",
    "from sklearn import*\n",
    "import numpy as np\n",
    "import numpy\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.models import Model\n",
    "from keras.layers import add,Input,Conv1D,Activation,Lambda,Dense,Flatten\n",
    "import inspect\n",
    "from typing import List\n",
    "from tensorflow.keras import backend as K, Model, Input, optimizers\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Activation, SpatialDropout1D, Lambda\n",
    "from tensorflow.keras.layers import Layer, Conv1D, Dense, BatchNormalization, LayerNormalization\n",
    "from keras.layers import Layer\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import keras.backend as K\n",
    "from keras.layers import Multiply,Dropout,Bidirectional,LSTM\n",
    "from keras.layers.core import *\n",
    "from keras.models import *\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py as hp\n",
    "import math\n",
    "from keras import optimizers\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import scipy.io as sci  \n",
    "from pandas import read_csv\n",
    "from datetime import datetime\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM,GRU\n",
    "import  pandas as pd\n",
    "import  os\n",
    "import time\n",
    "from keras.models import Sequential, load_model\n",
    "from sklearn import*\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "import random\n",
    "from keras import layers\n",
    "from keras.layers.core import Lambda\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "filename='./Target intention recognition dataset.csv'\n",
    "dataset = pd.read_csv('Target intention recognition dataset.csv', engine='python')\n",
    "\n",
    "dataset1 = pd.read_csv(filename,usecols=[4,5,6,7,8,9,10,11,12,13,15,17],engine='python')\n",
    "ndarray1=dataset1.values\n",
    "Features=ndarray1[:,0:12]\n",
    "\n",
    "ndarray=dataset.values\n",
    "minmax_scale = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "scaledFeatures=minmax_scale.fit_transform(Features)\n",
    "slid=12\n",
    "label=[]\n",
    "a=ndarray[:,3]\n",
    "num=len(ndarray)-slid+1\n",
    "for i in range(num):\n",
    "    c=sum(a[i:i+slid])/slid\n",
    "    if (c<=50):\n",
    "        d=0\n",
    "    elif(c>50)and(c<=80):\n",
    "        d=1\n",
    "    elif(c>80)and(c<=110):\n",
    "        d=2\n",
    "    elif(c>110)and(c<=150):\n",
    "        d=3\n",
    "    elif(c>150)and(c<=200):\n",
    "        d=4\n",
    "    else:\n",
    "        d=5\n",
    "    label.append(d)\n",
    "scale=0.8\n",
    "s=int(scale*num)\n",
    "trainX=[]\n",
    "testX=[]\n",
    "for i in range(s):\n",
    "    e=scaledFeatures[i:slid+i,:]\n",
    "    trainX.append(e)\n",
    "for i in range(s,num):\n",
    "    e=scaledFeatures[i:slid+i,:]\n",
    "    testX.append(e)\n",
    "from keras.utils import to_categorical\n",
    "onehot = to_categorical(label, num_classes=7)\n",
    "trainY=onehot[:s,:]\n",
    "testY=onehot[s:,:]\n",
    "trainX=numpy.array(trainX)\n",
    "trainY=numpy.array(trainY)\n",
    "testX=numpy.array(testX)\n",
    "testY=numpy.array(testY)\n",
    "trainX =trainX.reshape(s, slid, 12)\n",
    "trainY=trainY.reshape(s,7)\n",
    "testX=testX.reshape(num-s,slid,12)\n",
    "testY=testY.reshape(num-s,7)\n",
    "print(trainX.shape,trainY.shape,testX.shape,testY.shape)\n",
    "\n",
    "def attention_3d_block(inputs):\n",
    "    input_dim = int(inputs.shape[2])\n",
    "    a = Permute((2, 1))(inputs)\n",
    "    a = Reshape((input_dim, TIME_STEPS))(a)\n",
    "    a = Dense(TIME_STEPS, activation='softmax')(a)\n",
    "    if SINGLE_ATTENTION_VECTOR:\n",
    "        a = Lambda(lambda x: K.mean(x, axis=1), name='dim_reduction')(a)\n",
    "        a = RepeatVector(input_dim)(a)\n",
    "    a_probs = Permute((2, 1), name='attention_vec')(a)\n",
    "    output_attention_mul = Multiply()([inputs, a_probs])\n",
    "    return output_attention_mul\n",
    "  \n",
    "def model_attention_applied_after_lstm():\n",
    "    K.clear_session()\n",
    "    inputs = Input(shape=(TIME_STEPS, INPUT_DIM,))\n",
    "    lstm1 = Bidirectional(GRU(units=256, return_sequences=True))(inputs)\n",
    "    drop1 = Dropout(0.5)(lstm1)\n",
    "    lstm2 = Bidirectional(GRU(units=128,return_sequences=True))(drop1)\n",
    "    drop2 = Dropout(0.5)(lstm2)\n",
    "    lstm3 = Bidirectional(GRU(units=128, return_sequences=True))(drop2)\n",
    "    drop3 = Dropout(0.5)(lstm3)\n",
    "    attention_mul = attention_3d_block(drop3)\n",
    "    attention_mul = Flatten()(attention_mul)\n",
    "    output = Dense(7, activation='softmax')(attention_mul)\n",
    "    model = Model(input=[inputs], output=output)\n",
    "    return model\n",
    "    \n",
    "def model_attention_applied_before_lstm():\n",
    "    K.clear_session()\n",
    "    inputs = Input(shape=(TIME_STEPS, INPUT_DIM,))\n",
    "    attention_mul = attention_3d_block(inputs)\n",
    "    \n",
    "    attention_mul = LSTM(lstm_units, return_sequences=False)(attention_mul)\n",
    "    output = Dense(7, activation='softmax')(attention_mul)\n",
    "    model = Model(input=[inputs], output=output)\n",
    "    return model\n",
    "SINGLE_ATTENTION_VECTOR = True\n",
    "APPLY_ATTENTION_BEFORE_LSTM = False\n",
    "INPUT_DIM = 12\n",
    "TIME_STEPS = 12\n",
    "epochs = 200\n",
    "batch_size = 512\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_x = trainX\n",
    "    train_y = trainY\n",
    "    test_x = testX\n",
    "    test_y = testY\n",
    "    starttime=int(time.time())\n",
    "    m.compile('Nadam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "    m.summary()\n",
    "    filepath='stabc.best.hdf5'\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_categorical_accuracy', verbose=1, save_best_only=True,mode='max')\n",
    "    callbacks_list = [checkpoint]\n",
    "    history=m.fit(train_x, train_y,validation_data=(testX, testY),epochs=epochs, batch_size=batch_size,callbacks=callbacks_list, verbose=2)\n",
    "    endtime=int(time.time())\n",
    "    mod = load_model('stabc.best.hdf5')\n",
    "    predictions = mod.predict(test_x)\n",
    "    conf = confusion_matrix(test_y.argmax(axis=1), predictions.argmax(axis=1))\n",
    "    print(conf)\n",
    "\n",
    "def attention_3d_block(inputs):\n",
    "    input_dim = int(inputs.shape[2])\n",
    "    a = inputs\n",
    "    a = Dense(input_dim, activation='softmax')(a)\n",
    "    if SINGLE_ATTENTION_VECTOR:\n",
    "        a = Lambda(lambda x: K.mean(x,axis=1),name='dim_reduction')(a)\n",
    "        a = RepeatVector(TIME_STEPS)(a)\n",
    "    a_probs = Permute((1, 2), name='attention_vec')(a)\n",
    "    output_attention_mul = Multiply()([inputs, a_probs])\n",
    "    return output_attention_mul\n",
    "\n",
    "SINGLE_ATTENTION_VECTOR = True\n",
    "TIME_STEPS = 12\n",
    "\n",
    "batch_size = 256\n",
    "epochs = 200\n",
    "filter_nums =7\n",
    "kernel_size = 4\n",
    "\n",
    "train_X, train_label, test_X, test_label =trainX,trainY,testX,testY\n",
    "A=np.zeros((6,200))\n",
    "B=np.zeros((6,200))\n",
    "C=np.zeros((6,200))\n",
    "D=np.zeros((6,200))\n",
    "\n",
    "\n",
    "for i  in range(5):\n",
    "    model = models.Sequential()\n",
    "    input_tensor = Input(shape=(12,12))\n",
    "    attention_mul = attention_3d_block(input_tensor)\n",
    "    x = stabc(nb_filters=filter_nums, kernel_size=kernel_size, dilations=[1, 2],return_sequences=True)(attention_mul)\n",
    "    GRU1 = layers.Bidirectional(layers.GRU(units=128, return_sequences=True))(x)\n",
    "    drop1 = Dropout(0.5)(GRU1 )\n",
    "    GRU2 = layers.Bidirectional(layers.GRU(units=64, return_sequences=False))(drop1)\n",
    "out_put = layers.Dense(units=7, activation='softmax')(GRU2)\n",
    "    model = models.Model(inputs=input_tensor, outputs=[out_put])\n",
    "    model.summary()\n",
    "    filepath='weights.best.hdf5'\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_categorical_accuracy', verbose=1, save_best_only=True,mode='max')\n",
    "    callbacks_list = [checkpoint]\n",
    "    nadam=optimizers.Nadam(lr=0.004, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004)\n",
    "    model.compile(optimizer=nadam, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "    train_history=model.fit(train_X, train_label, validation_data=(test_X, test_label), epochs=epochs, verbose=2, callbacks=callbacks_list)\n",
    "    c=np.array(train_history.history['val_categorical_accuracy'])\n",
    "    d=np.array(train_history.history['val_loss'])\n",
    "    a=np.array(train_history.history['categorical_accuracy'])\n",
    "    b=np.array(train_history.history['loss'])  \n",
    "    A[i,:]=a\n",
    "    B[i,:]=b\n",
    "    C[i,:]=c\n",
    "    D[i,:]=d\n",
    "test_X[1:10].shape\n",
    "a=[]\n",
    "for i in range(100):\n",
    "    starttime =time.time()\n",
    "    predictions = model.predict(test_X[100:101])\n",
    "    endtime = time.time()\n",
    "    aa=(endtime-starttime)/1\n",
    "    a.append(aa)\n",
    "x=sum(a)/100\n",
    "print(x)\n",
    "fn5=(A[0,0,:]+A[0,1,:]+A[0,2,:])/3\n",
    "fn7=(A[1,0,:]+A[1,1,:]+A[1,2,:])/3\n",
    "fn10=(A[2,0,:]+A[2,1,:]+A[2,2,:])/3\n",
    "fn15=(A[3,0,:]+A[3,1,:]+A[3,2,:])/3\n",
    "fn20=(A[4,0,:]+A[4,1,:]+A[4,2,:])/3\n",
    "print(max(fn5),max(fn7),max(fn10),max(fn15),max(fn20))\n",
    "lfn5=(B[0,0,:]+B[0,1,:]+B[0,2,:])/3\n",
    "lfn7=(B[1,0,:]+B[1,1,:]+B[1,2,:])/3\n",
    "lfn10=(B[2,0,:]+B[2,1,:]+B[2,2,:])/3\n",
    "lfn15=(B[3,0,:]+B[3,1,:]+B[3,2,:])/3\n",
    "lfn20=(B[4,0,:]+B[4,1,:]+B[4,2,:])/3\n",
    "print(min(lfn5),min(lfn7),min(lfn10),min(lfn15),min(lfn20))\n",
    "plt.plot(fn7) \n",
    "kz2=(C[0,0,:]+C[0,1,:]+C[0,2,:])/3\n",
    "kz3=(C[1,0,:]+C[1,1,:]+C[1,2,:])/3\n",
    "kz4=(C[2,0,:]+C[2,1,:]+C[2,2,:])/3\n",
    "kz5=(C[3,0,:]+C[3,1,:]+C[3,2,:])/3\n",
    "kz6=(C[4,0,:]+C[4,1,:]+C[4,2,:])/3\n",
    "print(max(kz2),max(kz3),max(kz4),max(kz5),max(kz6))\n",
    "lkz2=(D[0,0,:]+D[0,1,:]+D[0,2,:])/3\n",
    "lkz3=(D[1,0,:]+D[1,1,:]+D[1,2,:])/3\n",
    "lkz4=(D[2,0,:]+D[2,1,:]+D[2,2,:])/3\n",
    "lkz5=(D[3,0,:]+D[3,1,:]+D[3,2,:])/3\n",
    "lkz6=(D[4,0,:]+D[4,1,:]+D[4,2,:])/3\n",
    "print(min(lkz2),min(lkz3),min(lkz4),min(lkz5),min(lkz6))\n",
    "\n",
    "List2=[128,256,512]\n",
    "train_X, train_label, test_X, test_label =trainX,trainY,testX,testY\n",
    "t=0\n",
    "E=np.zeros((3,3,200))\n",
    "F=np.zeros((3,3,200))\n",
    "for i  in List2:\n",
    "    hidden = i\n",
    "    for j in range(3):\n",
    "        model = models.Sequential()\n",
    "        input_tensor = Input(shape=(12,12))\n",
    "        attention_mul = attention_3d_block(input_tensor)\n",
    "        x = stabc(nb_filters=filter_nums, kernel_size=kernel_size, dilations=[1, 2], return_sequences=True)(attention_mul)\n",
    "        GRU1 = layers.Bidirectional(layers.GRU(units=hidden, return_sequences=False))(x)\n",
    "        out_put = layers.Dense(units=7, activation='softmax')(GRU1)\n",
    "        model = models.Model(inputs=input_tensor, outputs=[out_put])\n",
    "        model.summary()\n",
    "        filepath='stabc.best.hdf5'\n",
    "        checkpoint = ModelCheckpoint(filepath, monitor='val_categorical_accuracy', verbose=1, save_best_only=True,mode='max')\n",
    "        callbacks_list = [checkpoint]\n",
    "        model.compile(optimizer='Nadam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "        train_history=model.fit(train_X, train_label, validation_data=(test_X, test_label), epochs=epochs, verbose=2, callbacks=callbacks_list)\n",
    "        c=np.array(train_history.history['val_categorical_accuracy'])\n",
    "        d=np.array(train_history.history['val_loss'])\n",
    "    t=t+1\n",
    "\n",
    "hi128=(E[0,0,:]+E[0,1,:]+E[0,2,:])/3\n",
    "hi256=(E[1,0,:]+E[1,1,:]+E[1,2,:])/3\n",
    "hi512=(E[2,0,:]+E[2,1,:]+E[2,2,:])/3\n",
    "print(max(hi128),max(hi256),max(hi512))\n",
    "lhi128=(F[0,0,:]+F[0,1,:]+F[0,2,:])/3\n",
    "lhi256=(F[1,0,:]+F[1,1,:]+F[1,2,:])/3\n",
    "lhi512=(F[2,0,:]+F[2,1,:]+F[2,2,:])/3\n",
    "print(min(lhi128),min(lhi256),min(lhi512))\n",
    "\n",
    "for i  in range(3):\n",
    "        model = models.Sequential()\n",
    "        keras.backend.clear_session() \n",
    "        input_tensor = Input(shape=(12,12))\n",
    "        attention_mul = attention_3d_block(input_tensor)\n",
    "        x = stabc(nb_filters=filter_nums, kernel_size=kernel_size, dilations=[1, 2], return_sequences=True)(attention_mul)\n",
    "        GRU1 = layers.Bidirectional(layers.GRU(units=128, return_sequences=True))(x)\n",
    "        drop1 = Dropout(0.5)(GRU1)\n",
    "        GRU2 = layers.Bidirectional(layers.GRU(units=64, return_sequences=False))(drop1)\n",
    "        drop2 = Dropout(0.5)(GRU2)\n",
    "        out_put = layers.Dense(units=7, activation='softmax')(GRU2)\n",
    "        model = models.Model(inputs=input_tensor, outputs=[out_put])\n",
    "        model.summary()\n",
    "        filepath='stabc.best.hdf5'\n",
    "        checkpoint = ModelCheckpoint(filepath, monitor='val_categorical_accuracy', verbose=1, save_best_only=True,mode='max')\n",
    "        callbacks_list = [checkpoint]\n",
    "        model.compile(optimizer='Nadam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "        train_history=model.fit(train_X, train_label, validation_data=(test_X, test_label), epochs=epochs, verbose=2, callbacks=callbacks_list)\n",
    "        c=np.array(train_history.history['val_categorical_accuracy'])\n",
    "        d=np.array(train_history.history['val_loss'])\n",
    "\n",
    "hi6464=(G[0,0,:]+G[0,1,:]+G[0,2,:])/3\n",
    "hi12864=(G[1,0,:]+G[1,1,:]+G[1,2,:])/3\n",
    "hi128128=(G[2,0,:]+G[2,1,:]+G[2,2,:])/3\n",
    "print(max(hi6464),max(hi12864),max(hi128128))\n",
    "lhi6464=(H[0,0,:]+H[0,1,:]+H[0,2,:])/3\n",
    "lhi12864=(H[1,0,:]+H[1,1,:]+H[1,2,:])/3\n",
    "lhi128128=(H[2,0,:]+H[2,1,:]+H[2,2,:])/3\n",
    "print(min(lhi6464),min(lhi12864),min(lhi128128))\n",
    "\n",
    "def lizhu(x):\n",
    "    print('lizhubieku')\n",
    "    print('x=x+1')\n",
    "    return x\n",
    "\n",
    "print(lizhu(2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}